
def load_cora():
    num_nodes = 2708
    num_feats = 1433
    feat_data = np.zeros((num_nodes, num_feats))
    labels = np.empty((num_nodes, 2), dtype=np.int32)
    node_map = {}
    label_map = {}
    with open("cora\cora.content") as fp:
        for i,line in enumerate(fp):
            info = line.strip().split()
            feat_data[i,:] = list(map(float, info[1:-1]))
            node_map[info[0]] = i
            if not info[-1] in label_map:
                label_map[info[-1]] = len(label_map)
            labels[i][0] = label_map[info[-1]]
            labels[i][1] = info[0]

    #labels = labels[labels[:,1].argsort()] #ordino per numero di articolo, potrebbe non servire
    labels = np.delete(labels, 1, 1)
    

    adj_lists = defaultdict(set)
    with open("cora\cora.cites") as fp:
        for i,line in enumerate(fp):
            info = line.strip().split()
            paper1 = node_map[info[0]]
            paper2 = node_map[info[1]]
            adj_lists[paper1].add(paper2)
            adj_lists[paper2].add(paper1)

    
    return feat_data, labels, adj_lists

def process_input_data(feat_data, labels, adj_lists):
    adj_matrix = nx.adjacency_matrix(nx.from_dict_of_lists(adj_lists))
    #feat_data = feat_data[feat_data[:,0].argsort()] #ordino per numero di articolo, potrebbe non servire
    feat_data = sp.csr_matrix(feat_data)
    labels = to_categorical(labels)
    return feat_data, labels, adj_matrix



def split_dataset(labels):
    #creo vettori di indici per test, train e validation - sono fissati per ora
    idx_train = range(0, 140)
    idx_val = range(140, 640)
    idx_test = range(700, 2700)
    
    #preparo le maschere per test, train e validation
    train_mask = sample_mask(idx_train, labels.shape[0])
    val_mask = sample_mask(idx_val, labels.shape[0])
    test_mask = sample_mask(idx_test, labels.shape[0])

    y_train = np.zeros(labels.shape)
    y_val = np.zeros(labels.shape)
    y_test = np.zeros(labels.shape)

    #definisco le matrici di label per train, test e validation, sono matrici composte da vettori one-hot
    y_train[train_mask, :] = labels[train_mask, :]
    y_val[val_mask, :] = labels[val_mask, :]
    y_test[test_mask, :] = labels[test_mask, :]

    return (y_train, y_test, y_val, train_mask, test_mask, val_mask)

def to_sparse_adj(adj):
    sparse_adj=adj
    return sparse_adj

def load_data_old(dataset_str): 
    
    """
    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;
    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;
    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances
        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;
    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;
    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;
    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;
    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict
        object;
    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.

    All objects above must be saved using python pickle module.

    :param dataset_str: Dataset name
    :return: All data input files loaded (as well the training/test data).
    """
    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']
    objects = []
    for i in range(len(names)):
        with open("data/ind.{}.{}".format(dataset_str, names[i]), 'rb') as f:
            if sys.version_info > (3, 0):
                objects.append(pkl.load(f, encoding='latin1'))
                print("b")
            else:
                objects.append(pkl.load(f))
                print("a")

    (x, y, tx, ty, allx, ally, graph) = tuple(objects)
    #genero la lista dei nodi
    test_idx_reorder = parse_index_file("data/ind.{}.test.index".format(dataset_str))
    test_idx_range = np.sort(test_idx_reorder)

    if dataset_str == 'citeseer':
        # Fix citeseer dataset (there are some isolated nodes in the graph)
        # Find isolated nodes, add them as zero-vecs into the right position
        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)
        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))
        tx_extended[test_idx_range-min(test_idx_range), :] = tx
        tx = tx_extended
        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))
        ty_extended[test_idx_range-min(test_idx_range), :] = ty
        ty = ty_extended
    
    #estraggo le features
    features = sp.vstack((allx, tx)).tolil()
    features[test_idx_reorder, :] = features[test_idx_range, :]

    #definisco la matrice di adiacenza del grafo
    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))

    #estraggo le labels
    labels = np.vstack((ally, ty))
    labels[test_idx_reorder, :] = labels[test_idx_range, :]

    #creo vettori di indici per test, train e validation - sono fissati per ora
    idx_test = test_idx_range.tolist()
    idx_train = range(len(y))
    idx_val = range(len(y), len(y)+500)

    #preparo le maschere per test, train e validation
    train_mask = sample_mask(idx_train, labels.shape[0])
    val_mask = sample_mask(idx_val, labels.shape[0])
    test_mask = sample_mask(idx_test, labels.shape[0])

    y_train = np.zeros(labels.shape)
    y_val = np.zeros(labels.shape)
    y_test = np.zeros(labels.shape)

    #definisco le matrici di label per train, test e validation, sono matrici composte da vettori one-hot
    y_train[train_mask, :] = labels[train_mask, :]
    y_val[val_mask, :] = labels[val_mask, :]
    y_test[test_mask, :] = labels[test_mask, :]


    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask