
def load_cora():
    num_nodes = 2708
    num_feats = 1433
    feat_data = np.zeros((num_nodes, num_feats))
    labels = np.empty((num_nodes, 2), dtype=np.int32)
    node_map = {}
    label_map = {}
    with open("cora\cora.content") as fp:
        for i,line in enumerate(fp):
            info = line.strip().split()
            feat_data[i,:] = list(map(float, info[1:-1]))
            node_map[info[0]] = i
            if not info[-1] in label_map:
                label_map[info[-1]] = len(label_map)
            labels[i][0] = label_map[info[-1]]
            labels[i][1] = info[0]

    #labels = labels[labels[:,1].argsort()] #ordino per numero di articolo, potrebbe non servire
    labels = np.delete(labels, 1, 1)
    

    adj_lists = defaultdict(set)
    with open("cora\cora.cites") as fp:
        for i,line in enumerate(fp):
            info = line.strip().split()
            paper1 = node_map[info[0]]
            paper2 = node_map[info[1]]
            adj_lists[paper1].add(paper2)
            adj_lists[paper2].add(paper1)

    
    return feat_data, labels, adj_lists

def process_input_data(feat_data, labels, adj_lists):
    adj_matrix = nx.adjacency_matrix(nx.from_dict_of_lists(adj_lists))
    #feat_data = feat_data[feat_data[:,0].argsort()] #ordino per numero di articolo, potrebbe non servire
    feat_data = sp.csr_matrix(feat_data)
    labels = to_categorical(labels)
    return feat_data, labels, adj_matrix



def split_dataset(labels):
    #creo vettori di indici per test, train e validation - sono fissati per ora
    idx_train = range(0, 140)
    idx_val = range(140, 640)
    idx_test = range(700, 2700)
    
    #preparo le maschere per test, train e validation
    train_mask = sample_mask(idx_train, labels.shape[0])
    val_mask = sample_mask(idx_val, labels.shape[0])
    test_mask = sample_mask(idx_test, labels.shape[0])

    y_train = np.zeros(labels.shape)
    y_val = np.zeros(labels.shape)
    y_test = np.zeros(labels.shape)

    #definisco le matrici di label per train, test e validation, sono matrici composte da vettori one-hot
    y_train[train_mask, :] = labels[train_mask, :]
    y_val[val_mask, :] = labels[val_mask, :]
    y_test[test_mask, :] = labels[test_mask, :]

    return (y_train, y_test, y_val, train_mask, test_mask, val_mask)

def to_sparse_adj(adj):
    sparse_adj=adj
    return sparse_adj

def load_data_old(dataset_str): 
    
    """
    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;
    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;
    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances
        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;
    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;
    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;
    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;
    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict
        object;
    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.

    All objects above must be saved using python pickle module.

    :param dataset_str: Dataset name
    :return: All data input files loaded (as well the training/test data).
    """
    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']
    objects = []
    for i in range(len(names)):
        with open("data/ind.{}.{}".format(dataset_str, names[i]), 'rb') as f:
            if sys.version_info > (3, 0):
                objects.append(pkl.load(f, encoding='latin1'))
                print("b")
            else:
                objects.append(pkl.load(f))
                print("a")

    (x, y, tx, ty, allx, ally, graph) = tuple(objects)
    #genero la lista dei nodi
    test_idx_reorder = parse_index_file("data/ind.{}.test.index".format(dataset_str))
    test_idx_range = np.sort(test_idx_reorder)

    if dataset_str == 'citeseer':
        # Fix citeseer dataset (there are some isolated nodes in the graph)
        # Find isolated nodes, add them as zero-vecs into the right position
        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)
        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))
        tx_extended[test_idx_range-min(test_idx_range), :] = tx
        tx = tx_extended
        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))
        ty_extended[test_idx_range-min(test_idx_range), :] = ty
        ty = ty_extended
    
    #estraggo le features
    features = sp.vstack((allx, tx)).tolil()
    features[test_idx_reorder, :] = features[test_idx_range, :]

    #definisco la matrice di adiacenza del grafo
    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))

    #estraggo le labels
    labels = np.vstack((ally, ty))
    labels[test_idx_reorder, :] = labels[test_idx_range, :]

    #creo vettori di indici per test, train e validation - sono fissati per ora
    idx_test = test_idx_range.tolist()
    idx_train = range(len(y))
    idx_val = range(len(y), len(y)+500)

    #preparo le maschere per test, train e validation
    train_mask = sample_mask(idx_train, labels.shape[0])
    val_mask = sample_mask(idx_val, labels.shape[0])
    test_mask = sample_mask(idx_test, labels.shape[0])

    y_train = np.zeros(labels.shape)
    y_val = np.zeros(labels.shape)
    y_test = np.zeros(labels.shape)

    #definisco le matrici di label per train, test e validation, sono matrici composte da vettori one-hot
    y_train[train_mask, :] = labels[train_mask, :]
    y_val[val_mask, :] = labels[val_mask, :]
    y_test[test_mask, :] = labels[test_mask, :]


    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask


























     0,    38,    62,    88,   113,   137,   162,   189,   278,
          302,   335,   340,   355,   398,   440,   477,   533,   574,
          613,   616,   652,   695,   737,   777,   820,   862,   903,
          941,   965,   988,  1023,  1062,  1101,  1141,  1150,  1174,
         1217,  1260,  1361,  1386,  1411,  1459,  1505,  1551,  1597,
         1644,  1689,  1720,  1753,  1787,  1797,  1825,  1865,  1882,
         1901,  1909,  1928,  1939,  1961,  1980,  1991,  2001,  2041,
         2075,  2105,  2130,  2156,  2187,  2226,  2255,  2284,  2323,
         2364,  2405,  2448,  2469,  2489,  2506,  2524,  2545,  2564,
         2598,  2622,  2646,  2682,  2716,  2756,  2797,  2836,  2877,
         2916,  2954,  2991,  3026,  3061,  3094,  3113,  3146,  3181,
         3212,  3218,  3264,  3307,  3367,  3400,  3434,  3452,  3492,
         3531,  3567,  3584,  3611,  3663,  3716,  3742,  3770,  3813,
         3860,  3957,  3970,  3993,  4036,  4051,  4142,  4157,  4172,
         4205,  4217,  4244,  4256,  4271,  4290,  4307,  4325,  4358,
         4372,  4376,  4399,  4416,  4455,  4469,  4482,  4497,  4537,
         4557,  4578,  4618,  4659,  4699,  4739,  4769,  4792,  4804,
         4813,  4827,  4846,  4859,  4868,  4909,  4922,  4945,  4968,
         4987,  5000,  5018,  5033,  5056,  5100,  5143,  5188,  5213,
         5262,  5288,  5335,  5384,  5410,  5459,  5504,  5547,  5588,
         5627,  5668,  5710,  5753,  5794,  5839,  5864,  5909,  5930,
         5973,  6001,  6050,  6082,  6113,  6160,  6208,  6259,  6300,
         6356,  6419,  6454,  6484,  6510,  6567,  6625,  6653,  6676,
         6701,  6725,  6783,  6808,  6833,  6857,  6883,  6907,  6956,
         6984,  7000,  7045,  7075,  7097,  7132,  7163,  7204,  7259,
         7278,  7315,  7353,  7388,  7412,  7445,  7479,  7509,  7549,
         7584,  7621,  7643,  7650,  7673,  7692,  7711,  7746,  7779,
         7813,  7842,  7883,  7926,  7966,  7995,  8025,  8056,  8071,
         8104,  8140,  8165,  8183,  8202,  8219,  8239,  8259,  8281,
         8305,  8331,  8359,  8384,  8409,  8442,  8474,  8508,  8543,
         8576,  8609,  8654,  8695,  8736,  8775,  8806,  8845,  8890,
         8951,  8980,  8997,  9048,  9101,  9150,  9201,  9244,  9288,
         9330,  9370,  9409,  9472,  9533,  9630,  9685,  9810,  9937,
        10060, 10085, 10127